# レバテック案件スクレイピング設計書

---

## 1. このプログラムの目的

このプログラムは「レバテックフリーランス」という仕事サイトから、  
**Pythonに関係する案件を自動で集めてスプレッドシートに整理する** ためのツールです。

手作業で検索する代わりに、Pythonがブラウザを自動で操作してくれることで、  
- 案件を探す時間を短縮できる  
- 結果をスプレッドシートに整理できる  
- 将来的に自動で更新できる  

という効果があります。

---

## 2. 全体の流れ（動作のイメージ）

1. Chromeを自動で起動する  
2. レバテックの検索ページを開く  
3. 「Python」と「終了案件を除く」にチェックを入れる  
4. 「検索」ボタンをクリックする  
5. 検索結果の案件を1ページずつ読み取る（最大10ページ）  
6. 重複や条件に合わない案件を除外する  
7. スプレッドシートに結果を書き込む  

---

## 3. 使用するファイル構成

```
levtech_scraper/
├── main.py                 # 実行の入り口（プログラム全体の起動）
├── flow.py                 # 自動処理の流れをまとめる
├── chrome.py               # Chromeを開いたりURLを表示したりする
├── selenium_manager.py     # ページ内のボタンや文字を見つけて操作する
├── logger.py               # ログ（実行記録）を出力する
├── spreadsheet_manager.py  # スプレッドシートに書き込む
├── .env                    # URLなどの秘密情報を入れる
└── docs/
    └── design.md           # この設計書
```

---

## 4. クラスの役割（わかりやすく）

| クラス名 | 役割 |
|-----------|------|
| ChromeManager | Chromeを起動して、ページを開く |
| GetElement | ページの中からボタンや入力欄を探す |
| ActionElement | ボタンをクリックしたり、文字を入力したりする |
| AutoScrapingFlow | 全体の流れを管理する（司令塔） |
| SpreadsheetManager | 結果をスプレッドシートに保存する |

---

## 5. 集めるデータの内容

| 項目名 | 内容 |
|---------|------|
| タイトル | 案件の名前 |
| URL | 詳細ページのリンク |
| 最低単価 | 最低金額（円） |
| 最高単価 | 最高金額（円） |
| 平均単価 | 平均金額（円） |
| スキル | Pythonなどの使用技術 |
| 概要 | 案件の内容の一部（説明） |
| 取得日 | データを取得した日付 |

---

## 6. スプレッドシートの構成（書き込み先）

| 列 | 項目名 | 内容 | 型 |
|----|----------|------|------|
| A | index | 通し番号 | int |
| B | 取得日 | データ取得日 | str |
| C | 案件名（タイトル） | 案件タイトル | str |
| D | 雇用形態 | 業務委託・派遣など | str |
| E | 地域 | 勤務地またはリモート | str |
| F | 必要経験年数 | 経験目安 | str |
| G | 月単価換算額 | 平均月額報酬 | float |
| H〜M | 各報酬項目 | 年収・月収・時給 | float |
| N〜T | 各スキル列（AWS, Djangoなど） | 案件本文に含まれる場合1 | bool |
| U | URL | 案件詳細ページへのリンク | str |



## 7. 検索キーワード

本ツールでは、Python関連の案件を対象とし、以下のキーワードで自動検索を行う。

| No | 検索ワード |
|----|--------------|
| 1 | Python |
| 2 | Python エンジニア |
| 3 | Python 自動化 |
| 4 | Python データ分析 |
| 5 | Django |
| 6 | FastAPI |

これらのキーワードを順番に検索し、結果を集計する。


---

## 8. 処理の流れ（少し詳しく）

1. Chromeを起動  
2. .envファイルに書かれたURLを開く  
3. チェックボックスに自動でチェックを入れる  
4. 検索ボタンをクリック  
5. 案件の一覧を取得  
6. ページ送りして繰り返す（最大10ページ）  
7. 重複データを削除  
8. スプレッドシートに書き込み  
9. 終了ログを出力  

---

## 9. エラーが起きたときの動作

| 起きる可能性のある問題 | 対応方法 |
|--------------------------|-----------|
| Chromeが開かない | エラーメッセージを出して停止する |
| ボタンが見つからない | スキップして次の操作へ進む |
| ページが開けない | 3回リトライしてダメなら停止 |
| スプレッドシート書き込み失敗 | リトライまたはスキップ |

---

## 10. 使うPythonライブラリ

```
selenium
python-dotenv
gspread
oauth2client
beautifulsoup4
pandas
```

---

## 11. 開発の順番（実装ステップ）

| ステップ | 内容 | 使用ファイル |
|-----------|------|---------------|
| STEP1 | 設計と環境構築 | design.md, venv |
| STEP2 | Chrome起動とURLアクセス | chrome.py, .env |
| STEP3 | 検索条件クリック | selenium_manager.py |
| STEP4 | 案件情報の取得 | flow.py |
| STEP5 | ページ送り（10ページ分） | flow.py |
| STEP6 | データ整形と重複除外 | flow.py |
| STEP7 | スプレッドシート出力 | spreadsheet_manager.py |
| STEP8 | 全体統合とテスト | main.py, flow.py |

---

## 12. 環境ファイル（.env）の設定項目

| 変数名 | 内容 | 例 |
|---------|------|----|
| TARGET_URL | レバテック検索ページのURL | https://freelance.levtech.jp/project/search/ |
| SPREADSHEET_ID | 出力先のスプレッドシートID | 1A2B3C4D5E6Fxxxx |
| GOOGLE_CREDENTIALS | Google認証ファイルのパス | credentials.json |


## 13. 実行のしかた

ターミナルで以下を入力する。

```
python main.py
```

ブラウザが自動で起動して、指定のサイトを開く。

---

## 14. 更新履歴

| 日付 | バージョン |
|------|-------------|
| 2025-10-25 | v1.0 |
